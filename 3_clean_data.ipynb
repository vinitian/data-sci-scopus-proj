{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook is used to clean papers, both from the given dataset and the ones from Scopus API.\n",
    "\n",
    "Before running, please configure the path for each dataset in its section. The default path assumes you have the given dataset in folder `Data 2018-2023` and the scraped dataset in folder `papers`.\n",
    "\n",
    "Alternatively, you can also directly download the zip file of cleaned papers from [here](https://drive.google.com/drive/folders/1zEumhkVybecdpP9SETxyF5gMEYWHQbdJ?usp=sharing) and skip running `1_add_papers.ipynb`, `2_add_variables.ipynb`, and `3_clean_data.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all file names\n",
    "path = './Data 2018-2023/2023/'\n",
    "fileNames = [f for f in listdir(path) if isfile(join(path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cleaned data\n",
    "for i in range(len(fileNames)):\n",
    "    with open(path+fileNames[i], 'r', encoding=\"utf8\") as file:\n",
    "        data = json.load(file)\n",
    "        if 'dc:title' not in data['abstracts-retrieval-response']['coredata']:\n",
    "            continue\n",
    "        print('creating: '+fileNames[i])\n",
    "        eid = data['abstracts-retrieval-response']['coredata']['eid']\n",
    "        title = data['abstracts-retrieval-response']['coredata']['dc:title']\n",
    "        yyyy_mm_dd = data['abstracts-retrieval-response']['coredata']['prism:coverDate']\n",
    "        subjAreas = data['abstracts-retrieval-response']['subject-areas']['subject-area']#[index]['@code']\n",
    "        citedByCount = str(data['abstracts-retrieval-response']['coredata']['citedby-count'] or '0')\n",
    "        \n",
    "        keywords = []\n",
    "        if data['abstracts-retrieval-response']['authkeywords'] is not None:\n",
    "            if type(data['abstracts-retrieval-response']['authkeywords']['author-keyword']) is not list:\n",
    "                keywords = data['abstracts-retrieval-response']['authkeywords']['author-keyword']['$'].split(', ')\n",
    "            else:\n",
    "                keywords = [k['$'] for k in data['abstracts-retrieval-response']['authkeywords']['author-keyword']]\n",
    "        \n",
    "        authors = []\n",
    "        for author in data['abstracts-retrieval-response']['authors']['author']:\n",
    "            authorName = str(author['preferred-name']['ce:given-name'] or '') +' '+ str(author['preferred-name']['ce:surname'] or '')\n",
    "            authors.append(authorName)\n",
    "\n",
    "        refCount = 0\n",
    "        refList = []\n",
    "        if data['abstracts-retrieval-response']['item']['bibrecord']['tail'] != None:\n",
    "            refCount = data['abstracts-retrieval-response']['item']['bibrecord']['tail']['bibliography']['@refcount']\n",
    "            if type(data['abstracts-retrieval-response']['item']['bibrecord']['tail']['bibliography']['reference']) is not list:\n",
    "                if 'ref-title' in data['abstracts-retrieval-response']['item']['bibrecord']['tail']['bibliography']['reference']['ref-info']:\n",
    "                    refList.append(data['abstracts-retrieval-response']['item']['bibrecord']['tail']['bibliography']['reference']['ref-info']['ref-title']['ref-titletext'])\n",
    "            else:\n",
    "                for AData in data['abstracts-retrieval-response']['item']['bibrecord']['tail']['bibliography']['reference']:\n",
    "                    if 'ref-title' in AData['ref-info']:\n",
    "                            refList.append(AData['ref-info']['ref-title']['ref-titletext'])\n",
    "        aff = data['abstracts-retrieval-response']['affiliation']\n",
    "\n",
    "        fund = []\n",
    "        if 'xocs:meta' in data['abstracts-retrieval-response']['item'] and 'xocs:funding' in data['abstracts-retrieval-response']['item']['xocs:meta']['xocs:funding-list']:\n",
    "            # print('have fund')\n",
    "            if type(data['abstracts-retrieval-response']['item']['xocs:meta']['xocs:funding-list']['xocs:funding']) is not list:\n",
    "                if 'xocs:funding-agency-matched-string' in data['abstracts-retrieval-response']['item']['xocs:meta']['xocs:funding-list']['xocs:funding']:\n",
    "                    fund.append(data['abstracts-retrieval-response']['item']['xocs:meta']['xocs:funding-list']['xocs:funding']['xocs:funding-agency-matched-string'])\n",
    "            else:\n",
    "                for a in data['abstracts-retrieval-response']['item']['xocs:meta']['xocs:funding-list']['xocs:funding']:\n",
    "                    if 'xocs:funding-agency-matched-string' in a:\n",
    "                        fund.append(a['xocs:funding-agency-matched-string'])\n",
    "\n",
    "    dictionary = {\n",
    "        'eid': eid,\n",
    "        'title': title,\n",
    "        'cover-date': yyyy_mm_dd,\n",
    "        'subject-areas': subjAreas,\n",
    "        'keywords': keywords,\n",
    "        'authors': authors,\n",
    "        'citedby-count': int(citedByCount),\n",
    "        'ref-count': int(refCount),\n",
    "        'ref-list': refList,\n",
    "        'affiliation':aff,\n",
    "        'funding': fund\n",
    "    }\n",
    "    \n",
    "    # Serializing json\n",
    "    json_object = json.dumps(dictionary, indent=4)\n",
    "\n",
    "    with open(\"./Data 2018-2023/clean/\"+fileNames[i], \"w\", encoding=\"utf8\") as outfile:\n",
    "        outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean scraped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all file names\n",
    "path = './papers/'\n",
    "fileNames = [f for f in listdir(path) if isfile(join(path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cleaned data\n",
    "for i in range(len(fileNames)):\n",
    "    with open(path+fileNames[i], 'r', encoding=\"utf8\") as file:\n",
    "        data = json.load(file)\n",
    "        if 'dc:title' not in data['abstracts-retrieval-response']['coredata']:\n",
    "            continue\n",
    "        if 'dc:creator' not in data['abstracts-retrieval-response']['coredata']:\n",
    "            continue\n",
    "        print('creating: '+fileNames[i])\n",
    "        eid = data['abstracts-retrieval-response']['coredata']['eid']\n",
    "        title = data['abstracts-retrieval-response']['coredata']['dc:title']\n",
    "        yyyy_mm_dd = data['abstracts-retrieval-response']['coredata']['prism:coverDate']\n",
    "        subjAreas = data['subject-areas']\n",
    "        citedByCount = str(data['abstracts-retrieval-response']['coredata']['citedby-count'] or '0')\n",
    "        \n",
    "        keywords = []\n",
    "        if 'keywords' in data:\n",
    "            keywords = data['keywords']\n",
    "        \n",
    "        authors = set() # some authors in dc:creator have many affiliation -> repeated names\n",
    "        for author in data['abstracts-retrieval-response']['coredata']['dc:creator']['author']:\n",
    "            authorName = str(author['preferred-name']['ce:given-name'] or '') +' '+ str(author['preferred-name']['ce:surname'] or '')\n",
    "            authors.add(authorName)\n",
    "\n",
    "        refCount = 0\n",
    "        if 'ref-count' in data:\n",
    "            refCount = data['ref-count']\n",
    "        refList = []\n",
    "        if 'ref-list' in data:\n",
    "            refList = data['ref-list']\n",
    "        aff = data['abstracts-retrieval-response']['affiliation']\n",
    "\n",
    "        fund = data['funding']\n",
    "\n",
    "    dictionary = {\n",
    "        'eid': eid,\n",
    "        'title': title,\n",
    "        'cover-date': yyyy_mm_dd,\n",
    "        'subject-areas': subjAreas,\n",
    "        'keywords': keywords,\n",
    "        'authors': list(authors),\n",
    "        'citedby-count': int(citedByCount),\n",
    "        'ref-count': int(refCount),\n",
    "        'ref-list': refList,\n",
    "        'affiliation':aff,\n",
    "        'funding': fund\n",
    "    }\n",
    "    \n",
    "    # Serializing json\n",
    "    json_object = json.dumps(dictionary, indent=4)\n",
    "\n",
    "    with open(\"./cleanPapers/\"+fileNames[i], \"w\", encoding=\"utf8\") as outfile:\n",
    "        outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, please move all the papers into one new folder and run this code to rename all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please insert your full path to the new folder\n",
    "path=\"./sorted_papers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "fileNames = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "for i in range(0, 29217):\n",
    "    with open(f\"{path}/{fileNames[i]}\", 'r', encoding=\"utf-8\") as file:\n",
    "        d = json.load(file)\n",
    "        number = \"0\"*(5 - len(str(i))) + str(i)\n",
    "        with open(f\"./sorted_papers/{number}.json\", \"w\", encoding=\"utf-8\") as newfile:\n",
    "            newfile.write(json.dumps(d, indent=4))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
